{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EndGame_Tuner.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPSCE+tqw8+dEKqaSrz6h2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dj-1087/BigContest-COIN/blob/master/EndGame_Tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C3sWWr-cNZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b36f2219-8e25-4690-b3b9-6e08de6524f8"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnbPMhY7cs6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "ba290d9e-fa20-4ce6-81ac-05684c392bc3"
      },
      "source": [
        "pip install -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/f7/4b41b6832abf4c9bef71a664dc563adb25afc5812831667c6db572b1a261/keras-tuner-1.0.1.tar.gz (54kB)\n",
            "\r\u001b[K     |██████                          | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.16.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-cp36-none-any.whl size=73200 sha256=f2e07a0558669d2ce0688ea088cd2394b10e1e77321b119f08c7bb852a5283ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/cc/62/52716b70dd90f3db12519233c3a93a5360bc672da1a10ded43\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=a59e1dadcb7aec8d5824fe8006a6d8f68c12a10599f16a7930ff8319dc9b4f19\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.3 keras-tuner-1.0.1 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo-FlomPcvGq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "1b72b79b-bfce-40ca-bc7d-627525f1ca74"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n",
        "from tensorflow.keras.layers import Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# returns train, inference_encoder and inference_decoder models\n",
        "def define_models(n_input, n_output, n_units , hp):\n",
        "\t# define training encoder\n",
        "\tencoder_inputs = Input(shape=(-1,20, n_input))\n",
        "\tencoder = LSTM(n_units, return_state=True,activation='relu')\n",
        "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\tencoder_states = [state_h, state_c]\n",
        "    # define training encoder2\n",
        "\tencoder_inputs2 = Input(shape=(-1, 20, n_input))\n",
        "\tencoder2 = LSTM(n_units, return_state=True, activation='relu')\n",
        "\tencoder_outputs2, state_h2, state_c2 = encoder2(encoder_inputs2)\n",
        "\tencoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "    lstm_inputs = Input(shape=(-1, 10, n_input))\n",
        "    decoder_inputs = LSTM(n_units, activation='relu')\n",
        "    decoder_inputs,  _, _  = decoder_inputs(lstm_inputs)\n",
        "\n",
        "\t# define training decoder\n",
        "\tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states, activation='relu')\n",
        "    decoder_timedistribute = TimeDistributed(Dense(hp.Int('units', min_value = 4, max_value = 64, step = 4), activation='relu'))\n",
        "    decoder_outputs = decoder_timedistribute(decoder_outputs)\n",
        "\n",
        " \t# define training decoder2\n",
        "\tdecoder_lstm2 = LSTM(n_input, return_sequences=True, return_state=True)\n",
        "\tdecoder_outputs2, _, _ = decoder_lstm2(decoder_outputs, initial_state=encoder_states2, activation='relu')\n",
        "    decoder_timedistribute2 = TimeDistributed(Dense(1, activation='sigmoid'))\n",
        "\toutput = decoder_timedistribute2(decoder_outputs2)\n",
        " \n",
        "\tmodel = Model(inputs=[encoder_inputs, encoder_inputs2, decoder_inputs], outputs=decoder_outputs2)\n",
        "\tdecoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\t# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\t# define inference encoder\n",
        "\tencoder_model = Model(encoder_inputs, encoder_states)\n",
        "\t# define inference decoder\n",
        "\tdecoder_state_input_h = Input(shape=(n_units,))\n",
        "\tdecoder_state_input_c = Input(shape=(n_units,))\n",
        "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\tdecoder_states = [state_h, state_c]\n",
        "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
        "\tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        " \n",
        "   \n",
        "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-3,1e-4,1e-5])),\n",
        "                  loss = 'binary_crossentropy',\n",
        "                  metrics=['binary_accuracy'])\n",
        "\t# return all models\n",
        "\treturn model, encoder_model, decoder_model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-6e2880d99087>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-3,1e-4,1e-5])),\u001b[0m\n\u001b[0m                                                                                      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlgW-rGgqTqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "train, infenc, infdec = define_models(n_features, n_features, 128)\n",
        "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "train.fit([X1, X2], y, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XysVyAhtxT4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate LSTM\n",
        "total, correct = 100, 0\n",
        "for _ in range(total):\n",
        "\tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
        "\ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
        "\tif array_equal(one_hot_decode(y[0]), one_hot_decode(target)):\n",
        "\t\tcorrect += 1\n",
        "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv6gGMXlUL_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns train, inference_encoder and inference_decoder models\n",
        "def define_models(n_input, n2_input, n_output, n_units):\n",
        "\t# define training encoder\n",
        "\tencoder_inputs = Input(shape=(None, n_input))\n",
        "\tencoder = LSTM(n_units, return_state=True)\n",
        "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\tencoder_states = [state_h, state_c]\n",
        "    # define training encoder2\n",
        "\tencoder_inputs2 = Input(shape=(None, n2_input))\n",
        "\tencoder2 = LSTM(n_units, return_state=True)\n",
        "\tencoder_outputs2, state_h2, state_c2 = encoder(encoder_inputs2)\n",
        "\tencoder_states2 = [state_h2, state_c2]\n",
        "\t# define training decoder\n",
        "\tdecoder_inputs = Input(shape=(None, n_output))\n",
        "\tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\tdecoder_dense = Dense(n_output, activation='softmax')\n",
        "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
        "    # define training decoder2\n",
        "\tdecoder_inputs2 = Input(shape=(None, decoder_outputs))\n",
        "\tdecoder_lstm2 = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "\tdecoder_outputs2, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states2)\n",
        "\tdecoder_dense2 = Dense(n_output, activation='softmax')\n",
        "\tdecoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs2)\n",
        "\t# define inference encoder\n",
        "\tencoder_model = Model(encoder_inputs, encoder_states)\n",
        "\t# define inference decoder\n",
        "\tdecoder_state_input_h = Input(shape=(n_units,))\n",
        "\tdecoder_state_input_c = Input(shape=(n_units,))\n",
        "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\tdecoder_states = [state_h, state_c]\n",
        "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
        "\tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\t# return all models\n",
        "\treturn model, encoder_model, decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIV8NJqgdfId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.layers import Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVH_PSX3dofP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_encoder(n_input, hp):\n",
        "    # define training encoder\n",
        "\tencoder_inputs = Input(shape=(None, n_input))\n",
        "\tencoder = LSTM(hp.Int('encoder_units', min_value=4, max_value=128, step=4), return_state=True, activation='relu')\n",
        "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\tencoder_states = [state_h, state_c]\n",
        "    return encoder_outputs, encoder_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g08aVGFrg-Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dencoder(decode_input, encoder_states, hp):\n",
        " \tdecoder_inputs = Input(shape=(None, decode_input))\n",
        "\tdecoder_lstm = LSTM(hp.Int('encoder_units', min_value=4, max_value=128, step=4), return_sequences=True, return_state=True)\n",
        "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\tdecoder_dense = Dense(decode_input, activation='softmax')\n",
        "\tdecoder_outputs = TimeDistributed(decoder_dense(decoder_outputs))\n",
        "    return decoder_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kxJ909DhqL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns train, inference_encoder and inference_decoder models\n",
        "def define_models(encode_input, encode2_input, decode_input, hp):\n",
        "\t# define training encoder\n",
        "\tencoder_outputs, encoder_states = build_encoder(encode_input, hp)\n",
        "    # define training encoder2\n",
        "\tencoder_outputs2, encoder_states2 =  build_encoder(encode2_input, hp)\n",
        "\t# define training decoder\n",
        "\tdecoder_outputs = build_dencoder(decode_input, encoder_states, hp)\n",
        "    # define training decoder2\n",
        "\tdecoder_outputs2 = build_dencoder(decode_outputs, encoder_states2, hp)\n",
        "\tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\t# define inference encoder\n",
        "\tencoder_model = Model(encoder_inputs, encoder_states)\n",
        "\t# define inference decoder\n",
        "\tdecoder_state_input_h = Input(shape=(n_units,))\n",
        "\tdecoder_state_input_c = Input(shape=(n_units,))\n",
        "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\tdecoder_states = [state_h, state_c]\n",
        "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
        "\tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\t# return all models\n",
        "\treturn model, encoder_model, decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "629F6d_udYYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_model(hp):\n",
        "    inputs = Input(shape=(28,28,1))\n",
        "    x = inputs\n",
        "\n",
        "    for i in range(hp.Int('n_layers',1,3)):\n",
        "        #필터 개수 탐색\n",
        "        x = Conv2D(filters=hp.Int('filters_'+str(i), 4, 64, step=8, default=16),\n",
        "                   kernel_size = (3,3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(strides=2)(x)\n",
        "\n",
        "    #GAP? GMP?\n",
        "    if hp.Choice('global_pooling',['max','avg']) == 'avg':\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "    else:\n",
        "        x = GlobalMaxPooling2D()(x)\n",
        "    \n",
        "    #은닉층의 개수 탐색\n",
        "    x = Dense(units=hp.Int('units', min_value = 16, max_value = 128, step = 16),\n",
        "              activation = 'relu')(x)\n",
        "    \n",
        "    #드롭아웃률 탐색\n",
        "    x = Dropout(hp.Choice('dropout_rate', values = [0.2, 0.3, 0.5]))(x)\n",
        "    x = Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-3,1e-4,1e-5])),\n",
        "                  loss = 'categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}